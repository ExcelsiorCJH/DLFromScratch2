{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap01 - 신경망 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 수학과 파이썬 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 행렬의 원소별 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([[1, 2, 3], \n",
    "              [4, 5, 6]])\n",
    "X = np.array([[0, 1, 2], \n",
    "              [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  5],\n",
       "       [ 7,  9, 11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W + X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  6],\n",
       "       [12, 20, 30]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise(point-wise)\n",
    "W * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 브로드캐스트 (Broadcast)\n",
    "\n",
    "- https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 20],\n",
       "       [30, 40]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "\n",
    "A * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\ 3 & 4\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "10 & 20\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\ 3 & 4\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "10 & 20 \\\\ 10 & 20\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "10 & 40 \\\\ 30 & 80\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 40],\n",
       "       [30, 80]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "b = np.array([10, 20])\n",
    "\n",
    "A * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 벡터의 내적과 행렬의 곱\n",
    "\n",
    "- $\\mathbf{x} = (x_1, \\dots, x_n)$, $\\mathbf{y} = (y_1, \\dots, y_n)$ 에 대하여, 백터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^{T} \\mathbf{y} = x_1y_1 + x_2y_2 + \\cdots + x_ny_n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 벡터의 내적은 직관적으로 '두 벡터가 얼마나 같은 방향을 향하고 있는가'를 나타낸다. 벡터의 길이가 1이라고 가정하고, 두 벡터가 완전히 같은 방향이면 두 벡터의 내적은 1이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터의 내적\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행렬의 곱\n",
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://github.com/rougier/numpy-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 신경망의 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 신경망 추론의 전체 그림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpur -> hidden\n",
    "W1 = np.random.randn(2, 4)  # 가중치\n",
    "b1 = np.random.randn(4)\n",
    "x = np.random.randn(10, 2)\n",
    "h = np.matmul(x, W1) + b1  # b1 은 브로드캐스팅 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.33462594,  1.83375388,  1.36724767, -0.78160224],\n",
       "       [-3.31796424,  1.07546246,  0.98015921, -0.49495683],\n",
       "       [-4.18341648,  1.1503361 ,  0.95845804, -0.80703702],\n",
       "       [-2.72641124,  0.01163217,  0.47651541,  0.09384387],\n",
       "       [-3.51206655,  1.12462725,  0.99186665, -0.57695321],\n",
       "       [-3.22412013,  0.76790655,  0.82920093, -0.35008666],\n",
       "       [-2.71258202, -0.89403187,  0.01377586,  0.43420399],\n",
       "       [-3.68717149,  1.61922506,  1.2329531 , -0.81787355],\n",
       "       [-3.10348124,  2.41349991,  1.68011143, -0.9206346 ],\n",
       "       [-3.31372714,  1.33131827,  1.111451  , -0.58843528]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비선형 변환 : 시그모이드 함수(sigmoid function)\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp{(-x)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03440223, 0.86220831, 0.79693511, 0.31397467],\n",
       "       [0.03496003, 0.74563434, 0.72713981, 0.37872656],\n",
       "       [0.01501737, 0.7595723 , 0.72281297, 0.30852225],\n",
       "       [0.06143276, 0.50290801, 0.6169247 , 0.52344376],\n",
       "       [0.02897084, 0.75484601, 0.72945646, 0.35963396],\n",
       "       [0.03826806, 0.68306787, 0.69618594, 0.41336141],\n",
       "       [0.06223499, 0.29027849, 0.50344391, 0.60687709],\n",
       "       [0.02443092, 0.83468823, 0.77433502, 0.30621523],\n",
       "       [0.04296389, 0.91785096, 0.84291929, 0.28482861],\n",
       "       [0.03510326, 0.79105861, 0.75239952, 0.35699395]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sigmoid(h)  # 활성화(activation)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden -> output\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "s = np.matmul(a, W2) + b2  # score 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.66883962e-01,  9.38922574e-02,  1.10333791e+00],\n",
       "       [-8.67869189e-02,  2.39539933e-01,  9.92154210e-01],\n",
       "       [-1.03642770e-01,  1.76227184e-01,  1.03129749e+00],\n",
       "       [ 2.36378452e-01,  5.47614350e-01,  8.13989934e-01],\n",
       "       [-9.82273246e-02,  2.17445018e-01,  1.00641342e+00],\n",
       "       [ 1.05545234e-03,  3.16776467e-01,  9.44196857e-01],\n",
       "       [ 5.38248858e-01,  7.85386036e-01,  6.62533868e-01],\n",
       "       [-2.18709051e-01,  1.10956765e-01,  1.08457452e+00],\n",
       "       [-3.72002672e-01,  2.47587644e-02,  1.17703439e+00],\n",
       "       [-1.54103014e-01,  1.85832801e-01,  1.02917877e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 계층으로 클래스화 및 순전파 구현 \n",
    "\n",
    "- 완전연결계층(fully connected layer)에 의한 변환은 기하학에서의 아핀(Affine) 변환에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 시그모이드(Sigmoid) 레이어 구현\n",
    "class Sigmoid:\n",
    "    '''Sigmoid Layer class\n",
    "    \n",
    "    Sigmoid layer에는 학습하는 params가 따로 없으므로 \n",
    "    인스턴스 변수인 params는 빈 리스트로 초기화\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"순전파(forward propagation) 메서드\n",
    "        Args:\n",
    "            x(ndarray): 입력으로 들어오는 값\n",
    "        Returns:\n",
    "            Sigmoid 활성화 값\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전연결계층(Affine) 구현\n",
    "class Affine:\n",
    "    '''FC layer'''\n",
    "    def __init__(self, W, b):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            W(ndarray): 가중치(weight)\n",
    "            b(ndarray): 편향(bias)\n",
    "        \"\"\"\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"순전파(forward propagation) 메서드\n",
    "        Args:\n",
    "            x(ndarray): 입력으로 들어오는 값\n",
    "        Returns:\n",
    "            out(ndarray): Wx + b\n",
    "        \"\"\"\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/basic_nn.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치와 편향 초기화\n",
    "        # input -> hidden\n",
    "        W1 = np.random.randn(I, H)  \n",
    "        b1 = np.random.randn(H)\n",
    "         # hidden -> output\n",
    "        W2 = np.random.randn(H, O) \n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        # 레이어 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.parmas = [layer.params for layer in self.layers]\n",
    "        # self.params = []\n",
    "        # for layer in self.layers:\n",
    "        #     self.params += layer.params\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.37208419, -1.2272713 , -0.71600047],\n",
       "       [-1.99217296, -1.11038922, -1.17477847],\n",
       "       [-2.4097767 , -0.97492006, -1.02192888],\n",
       "       [-2.11839519, -1.48960661, -0.51950315],\n",
       "       [-2.18613271, -0.8669607 , -1.35818199],\n",
       "       [-2.49563619, -0.91864415, -1.03032285],\n",
       "       [-2.92241833, -1.28789096, -0.19786216],\n",
       "       [-2.57245646, -0.69173756, -1.35170837],\n",
       "       [-2.26468526, -0.77741298, -1.44616699],\n",
       "       [-2.81967667, -0.84530049, -0.87213041]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3557911 , -0.66037635, -0.3281263 ],\n",
       "       [ 0.22745096, -0.14754727, -0.20193402],\n",
       "       [ 0.42144866, -0.37246175, -0.43626679],\n",
       "       [-0.49126246, -0.50797159, -0.20269287],\n",
       "       [ 0.10785604, -0.58505229, -0.47976242],\n",
       "       [-0.22429006, -0.41612883, -0.23535545],\n",
       "       [-0.13092219, -0.60470566, -0.42340892],\n",
       "       [ 0.65300882, -0.42005398, -0.51794339],\n",
       "       [-0.29888822, -0.24882564, -0.11788807],\n",
       "       [-0.31995078, -0.68040913, -0.3912852 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 신경망의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 손실 함수(Loss Function)\n",
    "\n",
    "- 신경망의 성능을 나타내는 척도로 손실(loss)을 사용\n",
    "\n",
    "- 신경망의 손실은 손실 함수를 사용해 구함 \n",
    "\n",
    "- 다중 클래스 분류(multi-class classification)의 경우 **교차 엔트로피 오차**(CEE, Cross Entropy Error)를 사용\n",
    "\n",
    "\n",
    "- Softmax 함수의 식은 다음과 같다. \n",
    "    - 즉 class가 $n$개 일 때, $k$ 번째 클래스의 확률 $y_k$ 를 구하는 식\n",
    "\n",
    "$$\n",
    "y_{k} = \\frac{\\exp{(s_k)}}{\\sum_{i=1}^{n}{\\exp{(S_i)}}}\n",
    "$$\n",
    "\n",
    "\n",
    "- Cross Entropy 식은 다음과 같다.\n",
    "     - $t_k$는 $k$번째 클래스에 해당하는 정답 레이블\n",
    "     - $\\log$는 밑을 $e$로 하는 로그\n",
    "     - 정답 레이블 $t_k$는 원-핫 벡터이기 때문에, 실질적으로 $1$에만 해당하는 인덱스만 계산 된다.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L &- -\\sum_{k}{\\left[ t_k \\log{y_k} + (1-t_k) \\log{(1-y_k)}\\right]} \\\\ \n",
    "&= - \\sum_{k}{t_k \\log{y_k}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax with Loss 레이어 \n",
    "\n",
    "- 해당 교재에서는 소프트맥스 함수와 교차 엔트로피 오차를 계산하는 레이어인 `SoftmaxWithLoss` 클래스를 구현하여 사용한다.\n",
    "\n",
    "- [밑바닥 부터 시작하는 딥러닝-1]의 5.6.3 에서 확인할 수 있다.\n",
    "\n",
    "- `common/layer.py`에서 `SoftmaxWithLoss` 클래스를 확인할 수 있다.\n",
    "\n",
    "![](./images/softmaxwitloss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
