{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap04 - Word2Vec 속도 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chap03](https://github.com/ExcelsiorCJH/DLFromScratch2/tree/master/Chap03-Word2Vec)에서 살펴본 CBOW 모델의 구조는 다음과 같은 문제가 있다.\n",
    "\n",
    "- 말뭉치(corpus)에 포함된 어휘 수가 많아지면 계산량이 커진다.\n",
    "\n",
    "이를 해결하기 위해 이번 장에서는 두 가지 개선을 추가한다.\n",
    "\n",
    "1. `Embedding` 레이어를 도입한다.\n",
    "2. 네거티브 샘플링(NEG, Negative Sampling)이라는 새로운 손실함수를 도입한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Word2Vec 개선 ①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chap03](https://github.com/ExcelsiorCJH/DLFromScratch2/tree/master/Chap03-Word2Vec)에서 구현한 CBOW 모델의 문제점은 아래의 그림에서 확인할 수 있듯이 예를 들어, 어휘가 100만개, 은닉층의 뉴런이 100개인 CBOW 모델의 경우에는 다음의 두 계산이 병목(bottleneck)이 된다.\n",
    "\n",
    "- 입력층의 원핫(one-hot) 표현과 가중치 행렬 $\\mathbf{W}_{\\text{in}}$의 곱 계산\n",
    "\n",
    "- 은닉층과 가중치 행렬 $\\mathbf{W}_{\\text{out}}$의 곱 및 `Softmax`(특히 분모)의 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cbow_big.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Embedding 계층\n",
    "\n",
    "각 단어(어휘)를 원핫 표현으로 변환한 다음 가중치 행렬을 곱해주는 작업은 **결과적으로 단지 각 단어에 해당하는 특정 행을 추출**하는 것 뿐이다. 따라서, 원핫 표현으로의 변환과 가중치 행렬 곱 계산은 사실상 필요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Embedding 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두 번째 행 가져오기\n",
    "# -> index=2에 해당하는 단어 벡터\n",
    "W[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15, 16, 17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 W로 부터 여러행을 \n",
    "# 한꺼번에 추출하는 예제\n",
    "idx = np.array([1, 0, 3, 0])\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer 구현\n",
    "# commons/layers.py\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Word2Vec 개선 ②\n",
    "\n",
    "4.1에서는 `Input - hidden`의 병목을 `Embedding`이라는 새로운 레이어를 도입해주면서 해결했고, 이번에는 `hidden - output`의 병목을 **Negative Sampling**을 통해 해결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 은닉층 이후 계산의 문제점\n",
    "\n",
    "아래의 그림처럼 100만개의 단어에 대해 Softmax를 구하게 되면 계산량이 많아지는 문제가 있다.\n",
    "\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\exp{(s_k)}}{\\sum_{i=1}^{1000000}{\\exp{(s_i)}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cbow_big2.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 다중 분류에서 이진 분류로\n",
    "\n",
    "Negative Sampling(NEG)의 핵심 아이디어는 **'이진 분류'**<sup>binary classificaton</sup>에 있다. 즉, '다중 분류<sup>multi-class classification</sup>'를 '이진 분류'로 근사하는 것이 Negative Sampling을 이해하는 데 중요한 포인트다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/neg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 시그모이드 함수와 교차 엔트로피 오차\n",
    "\n",
    "다중 분류의 경우에는 출력층에서 점수<sup>score</sup>를 확률로 변환할 때, 소프트맥스 함수를 사용하고, 이진 분류의 경우에는 시그모이드 함수를 사용한다. \n",
    "\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + \\exp{(-x)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수를 적용해 확률 $y$를 구한 후, 이 확률 $y$로 부터 손실(Loss)을 구한다.\n",
    "\n",
    "\n",
    "$$\n",
    "L = - \\left[ t \\log{y} + (1-t) \\log{(1-y)} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Sigmoid with Loss' 레이어의 역전파를 구하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial x} &= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t \\\\\n",
    "\\frac{\\partial L}{\\partial y} &= -\\frac{t}{y} + \\frac{1-t}{1-y} = \\frac{y-t}{y(1-y)} \\\\\n",
    "\\frac{\\partial y}{\\partial x} &= y(1-y)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/sigmoid02.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식에서 'Sigmoid with Loss'레이어에서의 역전파는 $y-t$ 즉, 오차가 앞의 계층으로 흘러가게 된다. 따라서, 오차가 크면 '크게'학습하고, 오차가 작으면 '작게'학습하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 다중 분류에서 이진 분류로(구현)\n",
    "\n",
    "![](./images/cbow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chap04/negative_sampling_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import collections\n",
    "from common.np import *\n",
    "from common.layers import Embedding, SigmoidWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
