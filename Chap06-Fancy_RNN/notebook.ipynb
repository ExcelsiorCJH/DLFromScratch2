{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap06 - 게이트가 추가된 RNN\n",
    "\n",
    "> [Chap05-순환신경망(RNN)](https://github.com/ExcelsiorCJH/DLFromScratch2/blob/master/Chap05-Recurrent_Neural_Network/notebook.ipynb)에서 살펴본 RNN 구조는 순환경로를 통해 과거의 정보를 기억할 수 있었다. 하지만, 실제로는 성능이 좋지 못한데 그 이유는 장기(long term)의존 관계를 잘 학습할 수 없기 때문이다.  이번 장에서는 이러한 RNN의 단점을 보완한 LSTM, GRU에 대해서 다룬다. LSTM이나 GRU에는 **'게이트'**(gate)라는 구조가 더해져 있는데, 이 게이트 덕분에 시계열 데이터의 장기 의존 관계를 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 RNN의 문제점\n",
    "\n",
    "- RNN의 문제점은 시계열 데이터의 장기 으존 관계(long-term dependency)를 학습하기 어렵다.\n",
    "\n",
    "- 그 원인은 BPTT에서 기울기 소실 또는 기울기 폭발(vanishing & exploding gradient)이 일어나기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 RNN 복습\n",
    "\n",
    "<img src=\"./images/fig_6-1.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 계층은 시계열 데이터인 $\\mathbf{x}_{t}$를 입력하면 $\\mathbf{h}_{t}$를 출력한다. \n",
    "\n",
    "- $\\mathbf{h}_{t}$는 RNN 계층의 **은닉 상태(hidden state)**라고 하며, 과거 정보를 저장하는 역할을 한다.\n",
    "\n",
    "- RNN cell의 내부를 자세히 살펴보면 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-2.png\" height=\"50%\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 기울기 소실 또는 기울기 폭발\n",
    "\n",
    "<img src=\"./images/fig_6-3.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 예시에서 `\"?\"`에 들어가는 단어는 \"Tom\"이다. \n",
    "\n",
    "- RNNLM이 이 문제에 올바르게 답하려면, \"Tom was watching TV in his room.\"과  \"Mary came into the room.\"이라는 정보를 기억해둬야 한다.\n",
    "\n",
    "- 즉, 이러한 정보를 RNN 레이어의 hidden state에 인코딩해 보관해둬야 한다.\n",
    "\n",
    "- 위의 예시에 대한 RNNLM의 BPTT는 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-4.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 RNN 레이어가 과거 방향으로 기울기(gradient)를 전달함으로써 시간 방향의 의존관계를 학습할 수 있게 된다.\n",
    "\n",
    "- 이때의 기울기는 (이론적으로) 학습해야 할 정보가 들어 있고, 그것을 과거로 전달함으로써 장기 의존 관계를 학습한다.\n",
    "\n",
    "- 하지만, 단순한 RNN(vanilla RNN) 레이어에서는 시간을 거슬러 올라갈수록 기울기가 작아지거나 커지는 문제가 발생한다(vanishing or exploding gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 기울기 소실과 기울기 폭발의 원인\n",
    "\n",
    "<img src=\"./images/fig_6-5.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림은 길이가 $T$인 시계열 데이터를 가정하여 $T$번째 정답 레이블로부터 전해지는 기울기가 어떻게 변화하는지 나타낸 그림이다.\n",
    "\n",
    "- 이때, 시간 방향 기울기(gradient)를 살펴보면, 역전파로 전해지는 기울기는 차례로 `'tanh'` → `'+'` → `'MatMul'` 연산을 통과한다. \n",
    "\n",
    "- `'+'`의 역전파는 상류에서 전해지는 기울기를 그대로 하류로 흘려보내기 때문에 기울기의 값이 변하지 않지만, `'tanh'`와 `'MatMul'`은 변하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh의 역전파\n",
    "\n",
    "$$\n",
    "y = \\tanh{(x)} = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\tanh{(x)}}{\\partial x} &= \\frac{( e^x + e^{-x})(e^x + e^{-x}) - ( e^x - e^{-x})( e^x - e^{-x})}{( e^x + e^{-x})^{2}} \\\\ \n",
    "&= 1 - \\frac{( e^x - e^{-x})( e^x - e^{-x})}{( e^x + e^{-x})^{2}} \\\\ \n",
    "&= 1 - \\left\\{ \\frac{( e^x - e^{-x})}{( e^x + e^{-x})} \\right\\}^{2} \\\\\n",
    "&= 1 - \\tanh{(x)}^{2} \\\\\n",
    "&= 1-y^{2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-6.png\" height=\"40%\" width=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그래프에서 점선이 $y=\\tanh{(x)}$의 미분이며, 그 값이 1.0 이하이고, $x$가 0으로 부터 멀어질수록 작아진다. \n",
    "\n",
    "- 따라서, 역전파에서 기울기가 $\\tanh$ 노드를 지날때 마다 값은 계속해서 작아지게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MatMul에서의 역전파\n",
    "\n",
    "<img src=\"./images/fig_6-7.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상류로부터 흘러온 기울기 $\\mathbf{dh}$ `'MatMul'`노드에서의 역전파는 $\\mathbf{dh} \\cdot \\mathbf{W_h}^{\\mathsf{T}}$라는 행렬 곱이 된다.\n",
    "\n",
    "- 이러한 행렬곱이 시계열 데이터의 time_step 만큼 반복한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32971447 0.30503349 0.08888152]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chap06/rnn_gradient_graph.py\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "plt.rc('font', family=font_name)\n",
    "\n",
    "\n",
    "N = 2  # 미니배치 크기\n",
    "H = 3  # hidden state 벡터의 차원 수\n",
    "T = 20  # 시계열 데이터의 길이(= timestep)\n",
    "\n",
    "dh = np.ones((N, H))\n",
    "np.random.seed(3)\n",
    "# Wh = np.random.randn(H, H) \n",
    "Wh = np.random.randn(H, H) * 0.5 \n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.matmul(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "    \n",
    "u, s, vh = np.linalg.svd(Wh)\n",
    "print(s)\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 기울기 폭발 대책\n",
    "\n",
    "- 기울기 폭발(exploding gradient) 해결책으로는 **기울기 클리핑(gradients clipping)**이라는 기법이 있다.\n",
    "\n",
    "- 기울기 클리핑 알고리즘을 의사코드로 나타내면 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\text{if } \\left\\| \\hat{g} \\right\\| \\ge threshold:\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{g} = \\frac{threshold}{\\left\\| \\hat{g} \\right\\|} \\hat{g}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 식에서 $\\hat{g}$는 신경망에서 사용되는 모든 매개변수의 기울기를 하나로 모은 것이다.\n",
    "    - 예를 들어, $\\mathbf{W}_{1}$과 $\\mathbf{W}_{2}$ 매개변수에 대한 기울기 $\\mathbf{dW}_{1}$과 $\\mathbf{dW}_{2}$를 결합(제곱의 합)한 것이다.\n",
    "    \n",
    "\n",
    "- 위의 식에서 $\\left\\| \\hat{g} \\right\\|$(기울기 L2 노름)가 $threshold$ 값을 초과하면 두 번째 수식과 같이 기울기를 수정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap06/clip_grads.py\n",
    "import numpy as np\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0  # threshold\n",
    "\n",
    "def clip_grads(grds, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [2.77977507 4.54622076 2.05410345 2.01378711 5.1403506  0.87229369\n",
      " 4.83585532 3.62176212 7.07686622]\n",
      "after: [0.66651711 1.09006443 0.4925201  0.4828533  1.23252117 0.20915313\n",
      " 1.15951119 0.86840351 1.69684679]\n"
     ]
    }
   ],
   "source": [
    "print('before:', dW1.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('after:', dW1.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 기울기 소실과 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 LSTM의 인터페이스\n",
    "\n",
    "- 아래의 그림과 같이 $\\tanh{\\left( \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}} + \\mathbf{x}_{t} \\mathbf{W}_{\\mathbf{x}} + \\mathbf{b} \\right)}$ 를 `tanh`노드로 나타내어 간소화한 것이다.\n",
    "\n",
    "<img src=\"./images/fig_6-10.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN Cell과 LSTM Cell을 간단하게 비교하면 아래의 그림처럼 나타낼 수 있다.\n",
    "\n",
    "<img src=\"./images/fig_6-11.png\" height=\"70%\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 보듯 LSTM 계층에는 $\\mathbf{c}$라는 경로가 있다는 것이 RNN과의 차이다. \n",
    "\n",
    "- $\\mathbf{c}$를 **기억 셀**(memory cell)이라고 하며, LSTM의 기억 메커니즘이다.\n",
    "\n",
    "- memory cell의 특징은 LSTM 계층 내에서만 주고받는다는 것이다.\n",
    "    - 다른 계층으로는 출력하지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 LSTM 계층 조립하기\n",
    "\n",
    "- $\\mathbf{c}_{t}$에는 timestep $t$에서의 LSTM의 기억이 저장되어 있는데, 과거로 부터 $t$까지에 필요한 모든 정보가 저장되어 있다고 가정한다.\n",
    "\n",
    "- 이러한 기억 $\\mathbf{c}_{t}$를 바탕으로 특정 연산을 거친 후 외부 계층과 다음 timestep $t+1$의 LSTM에 hidden state $\\mathbf{h}_{t}$를 출력한다. \n",
    "     - 이때 출력하는 $\\mathbf{h}_{t}$는 아래의 그림처럼 memory cell $\\mathbf{c}_{t}$의 값을 $\\tanh$ 함수로 변환한 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-12.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- memory cell $\\mathbf{c}_{t}$와 hidden state $\\mathbf{h}_{t}$의 관계는 $\\mathbf{c}_{t}$의 각 원소에 $\\tanh$ 함수를 적용한 것이다. \n",
    "\n",
    "- 즉, $\\mathbf{c}_{t}$와 $\\mathbf{h}_{t}$의 원소수는 같다는 뜻이고, 예를 들어, $\\mathbf{c}_{t}$의 원소 수가 100개면 $\\mathbf{h}_{t}$의 원소 수도 100개가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 게이트(gate)의 개념\n",
    "\n",
    "- 게이트는 **데이터의 흐름**을 **제어(control)**하는 역할을 한다.\n",
    "\n",
    "- 아래의 그림처럼 게이트의 열림 상태는 `0.0 ~ 1.0` 사이의 실수로 나타나며, '게이트를 얼마나 열까'라는 것도 데이터로 부터 (자동으로) 학습 한다.\n",
    "\n",
    "- 게이트의 열림 상태를 구할 때는 `sigmoid` 함수를 사용하는데, `sigmoid`함수의 출력이 0.0 ~ 1.0의 실수이기 떄문이다.\n",
    "\n",
    "<img src=\"./images/fig_6-14.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM의 게이트들\n",
    "\n",
    "<img src=\"./images/lstm.PNG\" height=\"80%\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 output 게이트\n",
    "\n",
    "\n",
    "- output 게이트는 hidde state($\\mathbf{h}_{t}$)의 출력을 담당한다.\n",
    "\n",
    "- output 게이트는 $\\tanh{(\\mathbf{c}_{t})}$의 각 원소에 대해 '그것이 다음 timestep의 hidde state($\\mathbf{h}_{t}$)에 얼마나 중요한가'를 제어한다.\n",
    "\n",
    "- output 게이트의 열림 상태는 입력 $\\mathbf{x}_{t}$와 이전 상태 $\\mathbf{h}_{t-1}$로 부터 구한다. 아래의 식에서 $\\sigma {()}$는 시그모이드 함수를 의미한다.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = \\sigma \\left( \\mathbf{x}_{t}\\mathbf{W}_{\\mathbf{x}}^{(o)} + \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}}^{(o)} + \\mathbf{b}^{(o)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-15.png\" height=\"60%\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 output 게이트의 열림 상태를 계산한 $\\mathbf{o}$는 $\\tanh{\\mathbf{c}_{t}}$의 원소별 곱(element-wise, point-wise, 또는 아다마르 곱)을 통해 $\\mathbf{h}_{t}$를 구하게 된다.\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_{t} = \\mathbf{o} \\odot \\tanh{(\\mathbf{c}_{t})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - $\\tanh$의 출력은 `-1.0 ~ 1.0`의 실수이다. 이 `-1.0 ~ 1.0`의 수치를 그 안에 인코딩된 **'정보'**의 강약(정도)를 표시한다고 해석할 수 있다.\n",
    "- 시그모이드 함수의 출력은 `0.0 ~ 1.0`의 실수이며, 데이터를 얼만큼 통과시킬지를 정하는 비율을 뜻한다.\n",
    "- 따라서, 게이트에서는 시그모이드 함수가 사용되고, 실질적인 '정보'를 지니는 데이터에는 $\\tanh$함수가 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
