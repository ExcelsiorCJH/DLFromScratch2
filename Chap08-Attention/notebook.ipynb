{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap08 - 어텐션(Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Chap08---어텐션(Attention)\" data-toc-modified-id=\"Chap08---어텐션(Attention)-1\">Chap08 - 어텐션(Attention)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Goals\" data-toc-modified-id=\"Goals-1.1\">Goals</a></span></li><li><span><a href=\"#8.1-어텐션의-구조\" data-toc-modified-id=\"8.1-어텐션의-구조-1.2\">8.1 어텐션의 구조</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.1.1-seq2seq의-문제점\" data-toc-modified-id=\"8.1.1-seq2seq의-문제점-1.2.1\">8.1.1 seq2seq의 문제점</a></span></li><li><span><a href=\"#8.1.2-Encoder-개선\" data-toc-modified-id=\"8.1.2-Encoder-개선-1.2.2\">8.1.2 Encoder 개선</a></span></li><li><span><a href=\"#8.1.3-Decoder-개선-①\" data-toc-modified-id=\"8.1.3-Decoder-개선-①-1.2.3\">8.1.3 Decoder 개선 ①</a></span><ul class=\"toc-item\"><li><span><a href=\"#기존의-seq2seq\" data-toc-modified-id=\"기존의-seq2seq-1.2.3.1\">기존의 seq2seq</a></span></li><li><span><a href=\"#개선된-seq2seq\" data-toc-modified-id=\"개선된-seq2seq-1.2.3.2\">개선된 seq2seq</a></span></li><li><span><a href=\"#맥락-벡터-$\\mathbf{c}$를-구하는-과정을-코드로-살펴보기\" data-toc-modified-id=\"맥락-벡터-$\\mathbf{c}$를-구하는-과정을-코드로-살펴보기-1.2.3.3\">맥락 벡터 $\\mathbf{c}$를 구하는 과정을 코드로 살펴보기</a></span></li><li><span><a href=\"#미니배치-처리용-가중합-구현-샘플-코드\" data-toc-modified-id=\"미니배치-처리용-가중합-구현-샘플-코드-1.2.3.4\">미니배치 처리용 가중합 구현 샘플 코드</a></span></li><li><span><a href=\"#가중합의-계산-그래프\" data-toc-modified-id=\"가중합의-계산-그래프-1.2.3.5\">가중합의 계산 그래프</a></span></li><li><span><a href=\"#가중합(WeightSum)-클래스-구현\" data-toc-modified-id=\"가중합(WeightSum)-클래스-구현-1.2.3.6\">가중합(WeightSum) 클래스 구현</a></span></li></ul></li><li><span><a href=\"#8.1.4-Decoder-개선-②\" data-toc-modified-id=\"8.1.4-Decoder-개선-②-1.2.4\">8.1.4 Decoder 개선 ②</a></span><ul class=\"toc-item\"><li><span><a href=\"#attention-weight-샘플코드\" data-toc-modified-id=\"attention-weight-샘플코드-1.2.4.1\">attention weight 샘플코드</a></span></li><li><span><a href=\"#attention-weight의-계산-그래프\" data-toc-modified-id=\"attention-weight의-계산-그래프-1.2.4.2\">attention weight의 계산 그래프</a></span></li><li><span><a href=\"#AttentionWeight-클래스-구현\" data-toc-modified-id=\"AttentionWeight-클래스-구현-1.2.4.3\">AttentionWeight 클래스 구현</a></span></li></ul></li><li><span><a href=\"#8.1.5-Decoder-개선-③\" data-toc-modified-id=\"8.1.5-Decoder-개선-③-1.2.5\">8.1.5 Decoder 개선 ③</a></span><ul class=\"toc-item\"><li><span><a href=\"#Attention-Layer-구성하기\" data-toc-modified-id=\"Attention-Layer-구성하기-1.2.5.1\">Attention Layer 구성하기</a></span></li><li><span><a href=\"#Attention-레이어가-반영된-seq2seq\" data-toc-modified-id=\"Attention-레이어가-반영된-seq2seq-1.2.5.2\">Attention 레이어가 반영된 seq2seq</a></span></li><li><span><a href=\"#TimeAttention-구현\" data-toc-modified-id=\"TimeAttention-구현-1.2.5.3\">TimeAttention 구현</a></span></li></ul></li></ul></li><li><span><a href=\"#8.2-어텐션을-갖춘-seq2seq-구현\" data-toc-modified-id=\"8.2-어텐션을-갖춘-seq2seq-구현-1.3\">8.2 어텐션을 갖춘 seq2seq 구현</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.2.1-Encoder-구현\" data-toc-modified-id=\"8.2.1-Encoder-구현-1.3.1\">8.2.1 Encoder 구현</a></span></li><li><span><a href=\"#8.2.2-Decoder-구현\" data-toc-modified-id=\"8.2.2-Decoder-구현-1.3.2\">8.2.2 Decoder 구현</a></span></li><li><span><a href=\"#8.2.3-seq2seq-구현\" data-toc-modified-id=\"8.2.3-seq2seq-구현-1.3.3\">8.2.3 seq2seq 구현</a></span></li></ul></li><li><span><a href=\"#8.3-어텐션-평가\" data-toc-modified-id=\"8.3-어텐션-평가-1.4\">8.3 어텐션 평가</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.3.1-날짜-형식-변환-문제\" data-toc-modified-id=\"8.3.1-날짜-형식-변환-문제-1.4.1\">8.3.1 날짜 형식 변환 문제</a></span></li><li><span><a href=\"#8.3.2-어텐션을-갖춘-seq2seq의-학습\" data-toc-modified-id=\"8.3.2-어텐션을-갖춘-seq2seq의-학습-1.4.2\">8.3.2 어텐션을 갖춘 seq2seq의 학습</a></span></li><li><span><a href=\"#8.3.3-어텐션-시각화\" data-toc-modified-id=\"8.3.3-어텐션-시각화-1.4.3\">8.3.3 어텐션 시각화</a></span></li></ul></li><li><span><a href=\"#8.4-어텐션에-관한-남은-이야기\" data-toc-modified-id=\"8.4-어텐션에-관한-남은-이야기-1.5\">8.4 어텐션에 관한 남은 이야기</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.4.1-양방향-RNN\" data-toc-modified-id=\"8.4.1-양방향-RNN-1.5.1\">8.4.1 양방향 RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#TimeBiLSTM-클래스-구현\" data-toc-modified-id=\"TimeBiLSTM-클래스-구현-1.5.1.1\">TimeBiLSTM 클래스 구현</a></span></li></ul></li><li><span><a href=\"#8.4.2-Attention-레이어-사용-방법\" data-toc-modified-id=\"8.4.2-Attention-레이어-사용-방법-1.5.2\">8.4.2 Attention 레이어 사용 방법</a></span></li><li><span><a href=\"#8.4.3-seq2seq-심층화와-skip-연결\" data-toc-modified-id=\"8.4.3-seq2seq-심층화와-skip-연결-1.5.3\">8.4.3 seq2seq 심층화와 skip 연결</a></span></li></ul></li><li><span><a href=\"#8.5-어텐션-응용\" data-toc-modified-id=\"8.5-어텐션-응용-1.6\">8.5 어텐션 응용</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.5.1-구글-신경망-기계-번역(GNMT)\" data-toc-modified-id=\"8.5.1-구글-신경망-기계-번역(GNMT)-1.6.1\">8.5.1 구글 신경망 기계 번역(GNMT)</a></span></li><li><span><a href=\"#8.5.2-Transformer\" data-toc-modified-id=\"8.5.2-Transformer-1.6.2\">8.5.2 Transformer</a></span></li></ul></li><li><span><a href=\"#8.6-정리\" data-toc-modified-id=\"8.6-정리-1.7\">8.6 정리</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- 어텐션이 무엇인가\n",
    "\n",
    "- 어텐션의 구조를 코드 수준에서 이해\n",
    "\n",
    "- 실전 문제에 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 어텐션의 구조\n",
    "\n",
    "- 어텐션 메커니즘을 사용하여 `seq2seq`에서 필요한 정보에만 **'주목'**할 수 있게 된다.\n",
    "\n",
    "- 또한, `seq2seq`가 가지고 있던 문제도 해결할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 seq2seq의 문제점\n",
    "\n",
    "- `seq2seq`에서는 Encoder가 시계열 데이터를 인코딩하고, 이 인코딩된 정보를 Decoder로 전달한다.\n",
    "\n",
    "- 이때 Encoder의 출력은 **'고정 길이 벡터'**였는데, 이 부분에 큰 문제점이 있다.\n",
    "\n",
    "- 고정 길이 벡터는 입력 데이터(문장)의 길이에 관계없이, 항상 **같은** 길이의 벡터로 변환한다.\n",
    "\n",
    "- 그렇기 때문에, 필요한 정보가 벡터에 다 담기지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-1.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Encoder 개선\n",
    "\n",
    "- 기존의 `seq2seq`는 마지막 timestep의 hidden state $\\mathbf{h}_{t}$만을 Decoder에 전달했다. \n",
    "\n",
    "- 이를 개선하기 위해 Encoder의 <u>출력의 길이를 입력 문장의 길에 맞춰</u> 바꿔주는 것이 좋다.\n",
    "\n",
    "- 아래의 그림과 같이 Encoder의 LSTM 레이어에서 모든 timestep의 hiddent state 벡터를 사용한다.\n",
    "\n",
    "- 각 timestep(각 단어)의 hidden state를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻을 수 있다. → '하나의 고정 길이 벡터'라는 제약으로부터 해방!\n",
    "\n",
    "> 대붑분의 딥러닝 프레임워크에서는 RNN 계열의 레이어를 초기화할 때, \n",
    "> - 모든 timestep에 대한 hidden state `return`\n",
    "> - 마지막 timestep에서만 hidden state `return`\n",
    "> 을 설정할 수 있다. [Keras](https://keras.io/layers/recurrent/)에서는 `return_sequences=True`를 통해 모든 timestep의 hidden state를 반환 받을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-2.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 주목해야할 부분은 LSTM레이어가 각 timestep마다 출력한 hidden state의 **'내용'**이다.\n",
    "\n",
    "- 각 timestep $t$의 hidden state $\\mathbf{h}_{t}$에는 해당 timestep $t$에 입력된 단어의 정보가 많이 포함되어 있다.\n",
    "\n",
    "- Encoder가 출력하는 $\\mathbf{hs}$행렬은 아래의 그림과 같이 각 단어에 해당하는 벡터들의 집합으로 볼 수 있다.\n",
    "\n",
    "> 예를들어, 위의 그림에서처럼 $t=3$에서 \"고양이\"라는 단어를 입력했을 때, $\\mathbf{h}_{3}$은 \"고양이\"라는 단어의 영향을 가장 크게 받는다. 따라서, $\\mathbf{h}_{3}$은 \"고양이\"라는 단어의 *성분*이 많이 들어간 벡터라고 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-3.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 Decoder 개선 ①\n",
    "\n",
    "- Decoder는 아래의 그림과 같이 Encoder의 출력인 $\\mathbf{hs}$를 입력으로 받아 시계열 데이터로 변환하는 작업을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-4.png\" width=\"40%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기존의 seq2seq\n",
    "\n",
    "- 기존의 `seq2seq`는 Encoder에서 마지막 timestep의 hidden state만을 Decoder의 입력으로 사용했다. \n",
    "\n",
    "- 즉, Encoder의 LSTM 레이어의 **'마지막'** hidden state를 Decoder의 LSTM 레이어의 **'첫'** hidden state로 설정한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-5.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개선된 seq2seq\n",
    "\n",
    "- 기존의 `seq2seq`에서와는 달리 각 timestep $t$마다 hidden state의 행렬인 $\\mathbf{hs}$ 전부를 활용할 수 있도록 Decoder를 개선한다.\n",
    "\n",
    "> 사람이 문장을 번역할 때,\n",
    "> - *'나 = I', '고양이 = cat'* 이라는 것과 같이 '어떤 단어'에 주목하여 그 단어의 변환을 대응시킬 수 있다. \n",
    "> - 이러한 단어(혹은 문구)의 대응 관계를 나타내는 정보를 **얼라인먼트(alignment)**라 한다.\n",
    "> - 기존의 alignment는 사람이 수작업을 통해 구축했다면, Attention 매커니즘을 통해 alignment를 자동으로 구축할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 Decoder의 목표이며, 이러한 구조를 Attention이라 한다.\n",
    "\n",
    "- 구현하고자 하는 개선된 Decoder의 전체 틀은 아래의 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-6.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 ***'어떤 계산'***이 받는 입력은 두 가지로,\n",
    "    - 하나는 Encoder로 부터 받는 $\\mathbf{hs}$(각 timestep $t$의 hiddent state의 행렬)\n",
    "    - 다른 하나는 Decoder에서 각 timestep별 LSTM의 hidden state 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런다음 필요한 정보만 골라 위쪽의 Affine 레이어의 입력으로 집어 넣는다. \n",
    "\n",
    "- 위의 그림과 같이, 개선된 seq2seq의 Decoder에서 하고자 하는 것은 단어들의 alignment 추출이다.\n",
    "    - Decoder에서 출력된 단어와 대응관계인 단어의 벡터를 $\\mathbf{hs}$에서 찾는다는 의미\n",
    "    - 예를 들어, Decoder가 \"I\"를 출력할 때, $\\mathbf{hs}$에서 \"나\"에 대응하는 벡터를 선택\n",
    "    \n",
    "    \n",
    "- 이러한 **'선택'** 작업을 **'어떤 계산'**으로 수행하겠다는 것이 Attention의 목표이다.\n",
    "\n",
    "- 하지만, 이러한 *'선택'* 작업은 미분을 할 수 없으며, <u>미분이 불가능 하다</u>는 것은 backpropagtion을 통해 **학습을 시킬 수 없다**는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이를 미분 가능한 연산으로 대체하기 위한 방법으로는 '하나를 선택'하는 것이 아니라, **'모든 것을 선택'**하는 것이다.\n",
    "\n",
    "- 그리고 이때, 아래의 그림과 같이 각 단어의 **중요도(기여도)**를 나타내는 *'가중치'* 를 별도로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-7.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 각 단어의 중요도를 나타내는 '가중치' $\\mathbf{a}$를 이용한다. \n",
    "\n",
    "- $\\mathbf{a}$는 확률분포처럼 각 원소가 0.0 ~ 1.0 사이의 스칼라이며, 모든 원소의 총합은 1이 된다.\n",
    "\n",
    "- 그런 다음, 각 단어의 중요도를 나타내는 가중치 $\\mathbf{a}$와 각 단어의 벡터 $\\mathbf{hs}$로부터 가중합(weighted sum)을 구하여, Attention의 출력값, **맥락 벡터** $\\mathbf{c}$를 얻는다.\n",
    "\n",
    "> 맥락 벡터 $\\mathbf{c}$에는 현 시각(timestep)의 변환을 수행하는 데 필요한 정보가 담겨있으며, 이를 데이터로 부터 학습되게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-8.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 맥락 벡터 $\\mathbf{c}$를 구하는 과정을 코드로 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)  # Encoder의 output인 hs 랜덤하게 생성\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])  # attention distribution\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "# ar = a.reshape(5, 1)  # numpy broadcasting 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8 , 0.8 , 0.8 , 0.8 ],\n",
       "       [0.1 , 0.1 , 0.1 , 0.1 ],\n",
       "       [0.03, 0.03, 0.03, 0.03],\n",
       "       [0.05, 0.05, 0.05, 0.05],\n",
       "       [0.02, 0.02, 0.02, 0.02]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16302473,  1.02837243, -0.52674608, -0.48577415],\n",
       "       [-0.12523668, -0.63852466,  0.32367023, -2.0770472 ],\n",
       "       [ 0.72124699,  1.21389829, -0.74714707,  0.24942445],\n",
       "       [-1.29325626,  1.34291003,  2.05012806, -0.75353779],\n",
       "       [ 2.50445953, -0.15788155, -0.31239195, -1.48325245]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = hs * ar  # point-wise dot\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.sum(t, axis=0)  # 맥락 벡터 c\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12495991,  0.8592503 , -0.31518569, -0.65618325])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 미니배치 처리용 가중합 구현 샘플 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=-1)\n",
    "# ar = a.reshape(N, T, 1)  # 브로드캐스트를 사용하는 경우\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중합의 계산 그래프\n",
    "\n",
    "1. `Repeat`노드를 사용해 $\\mathbf{a}$를 복제하고, \n",
    "\n",
    "2. 이어서 `x` 노드로 원소별 곱을 계산한 다음\n",
    "\n",
    "3. `Sum`노드로 합을 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-11.png\" width=\"40%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중합(WeightSum) 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar= a.reshape(N, T, 1)#.repeat(H, axis=-1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)  # sum의 역전파 = repeat\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)  # repeat노드에 대한 역전퍄 = sum\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4 Decoder 개선 ②\n",
    "\n",
    "- 각 단어의 가중치 $\\mathbf{a}$를 구하는 방법을 살표보자. \n",
    "\n",
    "- 아래의 그림은 Decoder의 첫 번째 timestep에서의 LSTM 레이어가 hidden state를 출력하는 부분을 나타낸 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-12.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 Encoder에서의 출력을 $\\mathbf{h}_{s}$라 하고, Decoder의 LSTM의 출력(hidden state)을 $\\mathbf{h}_{t}$라고 정의 했다.\n",
    "\n",
    "- Attention의 목표는 $\\mathbf{h}_{t}$가 $\\mathbf{h}_{s}$의 각 단어 벡터와 얼마나 '비슷한가'를 수치로 나타내는 것이다. \n",
    "\n",
    "- 이를 나타내는 방법으로는 대표적으로 다음과 같이 3가지가 주로 사용된다.\n",
    "    - Basic dot-product attention: $\\mathbf{e}_{i} = \\mathbf{h}_{t}^{\\mathsf{T}} \\mathbf{h}_{s}$\n",
    "    - Multiplicative attention: $\\mathbf{e}_{i} = \\mathbf{h}_{t}^{\\mathsf{T}} \\mathbf{W} \\mathbf{h}_{s}$\n",
    "    - Additive attention: $\\mathbf{e}_{i} = v^{\\mathsf{T}} \\tanh{\\left(\\mathbf{W}_{t} \\mathbf{h}_{t} + \\mathbf{W}_{s}\\mathbf{h}_{s}\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 교재에서는 가장 단순한 방법인 벡터의 '내적'을 이용한다. 두 벡터 $\\mathbf{a} = (a_1, a_2, \\cdots, a_n)$과 $\\mathbf{b} = (b_1, b_2, \\cdots, b_n)$의 내적은 다음과 같이 계산한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\cdots + a_nb_n\n",
    "$$\n",
    "\n",
    "- '내적'의 직관적인 의미는 '두 벡터가 얼마나 같은 방향을 향하고 있는가'이다. 따라서, 두 벡터의 '유사도'를 표현하는 척도로 사용할 수 있다.\n",
    "\n",
    "- 아래의 그림은 벡터의 내적을 이용해 유사도를 산출해내는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-13.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention weight 샘플코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.shape : (10, 5, 4)\n",
      "s.shape : (10, 5)\n",
      "a.shape : (10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)  # encoder output\n",
    "h = np.random.randn(N, H)  # decoder h_t\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)  # hs와 h_t의 point-wise dot을 위한 처리\n",
    "\n",
    "t = hs * hr  # point-wise dot\n",
    "print(f't.shape : {t.shape}')\n",
    "\n",
    "s = np.sum(t, axis=-1)\n",
    "print(f's.shape : {s.shape}')\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(f'a.shape : {a.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention weight의 계산 그래프\n",
    "\n",
    "<img src=\"./images/fig_8-15.png\" width=\"40%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AttentionWeight 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5 Decoder 개선 ③"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Layer 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-17.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_layer.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention 레이어가 반영된 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-18.png\" width=\"70%\" height=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림에서 각 timestep $t$의 `Attention`레이어에는 `Encoder`의 출력인 $\\mathbf{hs}$가 입력된다. \n",
    "\n",
    "- 그리고, Decoder에서 LSTM레이어의 hidden state $\\mathbf{h}_{t}$ 벡터를 `Affine` 레이어에 context 벡터와 concat하여 입력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeAttention 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-20.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap08/attention_seq2seq.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from attention_layer import TimeAttention\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Decoder 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-21.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()  # Attention 레이어 \n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)  # context vector\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # context_vector & lstm h_t\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 날짜 형식 변환 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-22.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 151 / 351 | 시간 87[s] | 손실 1.62\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 175[s] | 손실 1.02\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 151 / 351 | 시간 85[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 170[s] | 손실 0.86\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "정확도 51.640%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 151 / 351 | 시간 86[s] | 손실 0.12\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 171[s] | 손실 0.02\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 151 / 351 | 시간 87[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 173[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 151 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 175[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 151 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 172[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.920%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 151 / 351 | 시간 85[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 170[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 151 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 172[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 151 / 351 | 시간 85[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 151 / 351 | 시간 85[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad, eval_interval=150)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 어텐션 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc('font', family=font_name, size=12)\n",
    "\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 어텐션에 관한 남은 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.1 양방향 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-30.png\" width=\"60%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bidirectional RNN(LSTM, GRU 등)에서는 정방향 LSTM 레이어에 *역방향* 으로 처리하는 LSTM 레이어를 추가한 형태를 말한다.\n",
    "\n",
    "- 그리고, 각 timestep $t$에서 정방향 & 역방향 LSTM의 hidden state $\\mathbf{h}_{t}$를 연결(concat, 또는 sum, average 등)시킨 벡터를 최종 $\\mathbf{h}_{t}$로 만든다.\n",
    "\n",
    "- 양방향으로 처리함으로써, 각 단어에 대응하는 hidden state 벡터에는 forward, backward 방양으로부터의 정보를 집약할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeBiLSTM 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common/time_layers.py\n",
    "\n",
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])  # backward를 위해 입력데이터 반전\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)  # forward, backward concat\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.2 Attention 레이어 사용 방법\n",
    "\n",
    "- Attention 레이어는 다양하게 조합되어 사용할 수 있다. \n",
    "\n",
    "- 다만, Attention 레이어를 조합하였을 때, 정확도에 대한 영향은 직접 데이터를 사용해 검증해보지 않으면 모른다. \n",
    "\n",
    "- 아래의 그림(출처: [CN Yah 블로그](http://cnyah.com/2017/08/01/attention-variants/))과 같이 Bahdanau Attention 메커니즘의 경우에는 LSTM 레이어가 context 벡터의 정보를 이용할 수 있도록 구성하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/attention-mechanisms.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4.3 seq2seq 심층화와 skip 연결\n",
    "\n",
    "- 번역 등 현실에서의 애플리케이션들은 풀어야 할 문제가 훨씬 복잡하다.\n",
    "\n",
    "- 이럴 경우에 층을 깊게 쌓은 seq2seq를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-33.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 층을 깊게 쌓을 때 사용되는 중요한 기법 중에 **skip connection**이 있다(residual connection 또는 short-cut).\n",
    "\n",
    "- skip connection에서는 2개의 출력이 하나로 **'더해'**지기 때문에 역전파 시 기울기가 **'그대로 흘려'** 보낸다.\n",
    "\n",
    "- 따라서, 층이 깊어져도 기울기 소실(또는 폭발)이 되지 않고 전파되어, 좋은 학습을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-34.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 어텐션 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.1 구글 신경망 기계 번역(GNMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_8-35.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/transformer.png\" width=\"65%\" height=\"65%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 정리\n",
    "\n",
    "- 어텐션은 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습한다.\n",
    "\n",
    "- 어텐션에서는 벡터의 내적을 사용해 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중합 벡터가 어텐션의 출력이 된다.\n",
    "\n",
    "- 어텐션에서 사용하는 연산은 미분 가능하기 때문에 오차역전파법으로 학습할 수 있다.\n",
    "\n",
    "- 어텐션이 산출하는 가중치(확률)을 시각화하면 입출력의 대응 관계를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
